{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Измалкова Дарья БКЛ182</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Домашнее задание №2 часть 1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Для обработки взята книга Ф.М. Достоевского \"Игрок\"</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('igrok.txt', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Парсинг с помощью Mystem</h3>\n",
    "<p>Питоновский модуль работал слишком медленно (больше нескольких часов), поэтому я воспользовалась консольной версией</p>\n",
    "<p>Опции \"-gin\" - печатать грамматическую информацию, склеивать информацию словоформ при одной лемме, каждое слово с новой строки</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.23 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "inp = \"igrok.txt\"\n",
    "outp = \"igrok.json\"\n",
    "mystem_path = os.path.join('/Users/User', 'mystem.exe')\n",
    "\n",
    "input_filename = os.path.abspath(inp)\n",
    "output_filename = os.path.abspath(outp)\n",
    "os.system(f\"{mystem_path} {'-gin'} {'--format json'} {input_filename} {output_filename}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Проверка, что файл нормально обработался и записался</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'analysis': [{'lex': 'сестра', 'gr': 'S,жен,од=(пр,ед|дат,ед)'}], 'text': 'сестре'}, {'analysis': [{'lex': 'быть', 'gr': 'V,нп=прош,ед,изъяв,сред,несов'}, {'lex': 'было', 'gr': 'PART='}], 'text': 'Было'}, {'analysis': [{'lex': 'ясно', 'gr': 'ADV='}, {'lex': 'ясный', 'gr': 'A=ед,кр,сред'}], 'text': 'ясно'}, {'analysis': [{'lex': 'что', 'gr': 'CONJ='}, {'lex': 'что', 'gr': 'SPRO,ед,сред,неод=(вин|им)'}, {'lex': 'что', 'gr': 'ADVPRO='}], 'text': 'что'}, {'analysis': [{'lex': 'они', 'gr': 'SPRO,мн=им'}], 'text': 'они'}]\n"
     ]
    }
   ],
   "source": [
    "ana_list = []\n",
    "with open('igrok.json', encoding='utf-8') as f:\n",
    "    text2 = f.read()\n",
    "    lines = text2.splitlines()\n",
    "\n",
    "for line in lines:\n",
    "    data = json.loads(line)\n",
    "    ana_list.append(data)\n",
    "\n",
    "print(ana_list[100:105])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Токенизация с помощью nltk</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(word_tokenize(text))\n",
    "# убрала пунктуацию и привела к нижнему регистру\n",
    "words = [w.lower() for w in word_tokenize(text) if w.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Парсинг с помощью pymorphy</h3>\n",
    "<p>Чтобы было удобнее потом сохранить результат в json, сразу перевожу данные в список из словарей</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "analist = []\n",
    "\n",
    "for word in words:\n",
    "    ana = morph.parse(word)\n",
    "    first = ana[0] # использую только 1-й вариант разбора\n",
    "\n",
    "    word = first.word # слово\n",
    "    tag = first.tag # грам. разбор \n",
    "    lemma = first.normal_form # лемма\n",
    "    prob = first.score # вероятность\n",
    "\n",
    "    ana_dict = {'text' : word, 'tag' : [tag], 'lemma' : lemma, 'prob' : prob}\n",
    "    analist.append(ana_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Чтобы можно было вытаскивать теги, не используя встроенные функции OpencorporaTag, добавила их отдельно в словарь, в список в значении \"tag\".</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'сестре', 'tag': [OpencorporaTag('NOUN,anim,femn sing,datv'), {'POS': 'NOUN', 'animacy': 'anim', 'case': 'datv', 'gender': 'femn', 'number': 'sing'}], 'lemma': 'сестра', 'prob': 0.545454}, {'text': 'было', 'tag': [OpencorporaTag('VERB,impf,intr neut,sing,past,indc'), {'POS': 'VERB', 'aspect': 'impf', 'gender': 'neut', 'mood': 'indc', 'number': 'sing', 'tense': 'past', 'transitivity': 'intr'}], 'lemma': 'быть', 'prob': 0.963576}, {'text': 'ясно', 'tag': [OpencorporaTag('ADVB,Prdx'), {'POS': 'ADVB'}], 'lemma': 'ясно', 'prob': 0.941176}, {'text': 'что', 'tag': [OpencorporaTag('CONJ'), {'POS': 'CONJ'}], 'lemma': 'что', 'prob': 0.791044}, {'text': 'они', 'tag': [OpencorporaTag('NPRO,3per,Anph plur,nomn'), {'POS': 'NPRO', 'case': 'nomn', 'number': 'plur', 'person': '3per'}], 'lemma': 'они', 'prob': 1.0}]\n"
     ]
    }
   ],
   "source": [
    "for parse in analist:\n",
    "    new_thing = {}\n",
    "    if parse['tag'][0].POS:\n",
    "        new_thing.update({'POS':parse['tag'][0].POS})\n",
    "    if parse['tag'][0].animacy:\n",
    "        new_thing.update({'animacy':parse['tag'][0].animacy})\n",
    "    if parse['tag'][0].aspect:\n",
    "        new_thing.update({'aspect':parse['tag'][0].aspect})\n",
    "    if parse['tag'][0].case:\n",
    "        new_thing.update({'case':parse['tag'][0].case})\n",
    "    if parse['tag'][0].gender:\n",
    "        new_thing.update({'gender':parse['tag'][0].gender})\n",
    "    if parse['tag'][0].involvement:\n",
    "        new_thing.update({'involvement':parse['tag'][0].involvement})\n",
    "    if parse['tag'][0].mood:\n",
    "        new_thing.update({'mood':parse['tag'][0].mood})\n",
    "    if parse['tag'][0].number:\n",
    "        new_thing.update({'number':parse['tag'][0].number})\n",
    "    if parse['tag'][0].person:\n",
    "        new_thing.update({'person':parse['tag'][0].person})\n",
    "    if parse['tag'][0].tense:\n",
    "        new_thing.update({'tense':parse['tag'][0].tense})\n",
    "    if parse['tag'][0].transitivity:\n",
    "        new_thing.update({'transitivity':parse['tag'][0].transitivity})\n",
    "    if parse['tag'][0].voice:\n",
    "        new_thing.update({'voice':parse['tag'][0].voice})\n",
    "    parse['tag'].append(new_thing)\n",
    "\n",
    "print(analist[100:105])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Пришлось оставить исходные значения \"tag\", так как не все грамматические значения относятся к какому-либо аттрибуту.</p>\n",
    "<p>Как результат, в значении \"tag\" список из двух елементов, 1-й - теги в формате OpencorporaTag, 2-й - словарь вида \"аттрибут : тег\".</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Сохранение в формате json</h3>\n",
    "<p>Формат json не признает OpencorporaTag, поэтому переделала тег в строку и убрала \"OpencorporaTag\", оставив только непосредственно теги</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('igrok_parse.json', 'w', encoding='utf-8') as f:\n",
    "\n",
    "    for parse in analist:\n",
    "        parse['tag'][0] = str(parse['tag'][0])\n",
    "        parse['tag'][0] = re.sub(r'OpencorporaTag()', '', parse['tag'][0])\n",
    "\n",
    "        json.dump(parse, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Проверка, что файл нормально записался, OpencorporaTag исчез.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'сестре', 'tag': ['NOUN,anim,femn sing,datv', {'POS': 'NOUN', 'animacy': 'anim', 'case': 'datv', 'gender': 'femn', 'number': 'sing'}], 'lemma': 'сестра', 'prob': 0.545454}, {'text': 'было', 'tag': ['VERB,impf,intr neut,sing,past,indc', {'POS': 'VERB', 'aspect': 'impf', 'gender': 'neut', 'mood': 'indc', 'number': 'sing', 'tense': 'past', 'transitivity': 'intr'}], 'lemma': 'быть', 'prob': 0.963576}, {'text': 'ясно', 'tag': ['ADVB,Prdx', {'POS': 'ADVB'}], 'lemma': 'ясно', 'prob': 0.941176}, {'text': 'что', 'tag': ['CONJ', {'POS': 'CONJ'}], 'lemma': 'что', 'prob': 0.791044}, {'text': 'они', 'tag': ['NPRO,3per,Anph plur,nomn', {'POS': 'NPRO', 'case': 'nomn', 'number': 'plur', 'person': '3per'}], 'lemma': 'они', 'prob': 1.0}]\n"
     ]
    }
   ],
   "source": [
    "ana_list = []\n",
    "with open('igrok_parse.json', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    lines = text.splitlines()\n",
    "\n",
    "for line in lines:\n",
    "    data = json.loads(line)\n",
    "    ana_list.append(data)\n",
    "\n",
    "print(ana_list[100:105])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Доля частей речи</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля NOUN 18.72%\n",
      "Доля PREP 9.6%\n",
      "Доля ADJF 9.23%\n",
      "Доля ADVB 7.61%\n",
      "Доля NPRO 11.49%\n",
      "Доля VERB 13.71%\n",
      "Доля NUMR 1.05%\n",
      "Доля CONJ 12.47%\n",
      "Доля PRCL 6.89%\n",
      "Доля INFN 2.8%\n",
      "Доля INTJ 0.23%\n",
      "Доля GRND 0.8%\n",
      "Доля PRED 0.51%\n",
      "Доля COMP 0.32%\n",
      "Доля ADJS 1.03%\n",
      "Доля PRTS 0.28%\n",
      "Доля PRTF 0.58%\n"
     ]
    }
   ],
   "source": [
    "word_count = len(analist) # количество слов в тексте\n",
    "poslist = [] # список частей речи\n",
    "\n",
    "# у некоторых слов отсутсвует часть речи (иноязычные слова, римские цифры)\n",
    "for parse in analist:\n",
    "    if 'POS' in parse['tag'][1].keys():\n",
    "        poslist.append(parse['tag'][1]['POS'])\n",
    "\n",
    "# считает количество вхождений\n",
    "posdict = Counter(poslist)\n",
    "# находит и выводит долю\n",
    "for pos, count in posdict.items():\n",
    "    print('Доля', pos, str(round(count/word_count * 100, 2))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Топ-20 глаголов и наречий</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-20 глаголов: ['быть', 'мочь', 'знать', 'сказать', 'хотеть', 'говорить', 'стать', 'думать', 'видеть', 'выйти', 'проиграть', 'заметить', 'дать', 'взять', 'любить', 'понимать', 'смотреть', 'отвечать', 'начать', 'выиграть']\n",
      "Топ-20 наречий: ['теперь', 'вдруг', 'уж', 'уже', 'очень', 'опять', 'здесь', 'тоже', 'тут', 'потому', 'тогда', 'почти', 'тотчас', 'наконец', 'более', 'совсем', 'потом', 'где', 'сейчас', 'действительно']\n"
     ]
    }
   ],
   "source": [
    "def top_pos(analist,tag):\n",
    "    lemmalist = [] # список лемм части речи\n",
    "    top_list = [] # список топ-20\n",
    "\n",
    "# находит лемму если в разборе соответсвующая часть речи   \n",
    "    for parse in analist:\n",
    "        if tag in parse['tag'][0]:\n",
    "            lemmalist.append(parse['lemma'])\n",
    "# cчитает вхождения\n",
    "    lemmadict = Counter(lemmalist)\n",
    "# находит 20 самых частотных\n",
    "    for key, value in lemmadict.most_common(20):\n",
    "        top_list.append(key)\n",
    "\n",
    "    return top_list\n",
    "    \n",
    "    \n",
    "print('Топ-20 глаголов:',top_pos(analist,'VERB'))\n",
    "print('Топ-20 наречий:',top_pos(analist,'ADVB'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>N-граммы</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(text, number):\n",
    "# делает список всех н-грамм\n",
    "    ngram_list = list(ngrams(text, number))\n",
    "# создает частотный словарь\n",
    "    ngrams_rank = Counter(ngram_list)\n",
    "# находит 25 самых частотных\n",
    "    top_20 = ngrams_rank.most_common(25)\n",
    "# убирает все лишние скобки\n",
    "    for ngram in top_20:\n",
    "        n_words = ' '.join(ngram[0])\n",
    "        frequency = ngram[1]\n",
    "        \n",
    "        print(n_words, frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Биграммы</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "что я 116\n",
      "mademoiselle blanche 96\n",
      "и не 89\n",
      "может быть 75\n",
      "я не 73\n",
      "мистер астлей 69\n",
      "всё это 55\n",
      "и в 54\n",
      "потому что 51\n",
      "на меня 47\n",
      "и я 47\n",
      "что вы 47\n",
      "у меня 45\n",
      "что она 44\n",
      "тотчас же 42\n",
      "ничего не 42\n",
      "не знаю 41\n",
      "и даже 41\n",
      "ко мне 40\n",
      "алексей иванович 40\n",
      "что он 38\n",
      "со мною 36\n",
      "я и 36\n",
      "да и 36\n",
      "и с 33\n"
     ]
    }
   ],
   "source": [
    "find_ngrams(words, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Триграммы</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "по крайней мере 27\n",
      "в самом деле 18\n",
      "до сих пор 18\n",
      "пятьдесят тысяч франков 15\n",
      "к тому же 12\n",
      "даже и не 12\n",
      "и в самом 11\n",
      "может быть и 10\n",
      "о том что 10\n",
      "на этот раз 10\n",
      "что я не 10\n",
      "я ничего не 10\n",
      "во всяком случае 9\n",
      "и mademoiselle blanche 9\n",
      "что же касается 9\n",
      "же касается до 9\n",
      "и уж конечно 8\n",
      "я так и 8\n",
      "mademoiselle blanche и 8\n",
      "а между тем 8\n",
      "в том что 8\n",
      "я не знаю 8\n",
      "в эту минуту 8\n",
      "я не могу 7\n",
      "для того чтобы 7\n"
     ]
    }
   ],
   "source": [
    "find_ngrams(words, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
