{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Измалкова Дарья БКЛ182</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Домашнее задание №2 часть 1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Для обработки взята книга Ф.М. Достоевского \"Игрок\"</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('igrok.txt', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Парсинг с помощью Mystem</h3>\n",
    "<p>Питоновский модуль работал слишком медленно (больше нескольких часов), поэтому я воспользовалась консольной версией</p>\n",
    "<p>Опции \"-gin\" - печатать грамматическую информацию, склеивать информацию словоформ при одной лемме, каждое слово с новой строки</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 947 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "inp = \"igrok.txt\"\n",
    "outp = \"igrok.json\"\n",
    "mystem_path = os.path.join('/Users/User', 'mystem.exe')\n",
    "\n",
    "input_filename = os.path.abspath(inp)\n",
    "output_filename = os.path.abspath(outp)\n",
    "os.system(f\"{mystem_path} {'-gin'} {'--format json'} {input_filename} {output_filename}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Проверка, что файл нормально обработался и записался</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'analysis': [{'lex': 'сестра', 'gr': 'S,жен,од=(пр,ед|дат,ед)'}], 'text': 'сестре'}, {'analysis': [{'lex': 'быть', 'gr': 'V,нп=прош,ед,изъяв,сред,несов'}, {'lex': 'было', 'gr': 'PART='}], 'text': 'Было'}, {'analysis': [{'lex': 'ясно', 'gr': 'ADV='}, {'lex': 'ясный', 'gr': 'A=ед,кр,сред'}], 'text': 'ясно'}, {'analysis': [{'lex': 'что', 'gr': 'CONJ='}, {'lex': 'что', 'gr': 'SPRO,ед,сред,неод=(вин|им)'}, {'lex': 'что', 'gr': 'ADVPRO='}], 'text': 'что'}, {'analysis': [{'lex': 'они', 'gr': 'SPRO,мн=им'}], 'text': 'они'}]\n"
     ]
    }
   ],
   "source": [
    "ana_list = []\n",
    "with open('igrok.json', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    lines = text.splitlines()\n",
    "\n",
    "for line in lines:\n",
    "    data = json.loads(line)\n",
    "    ana_list.append(data)\n",
    "\n",
    "print(ana_list[100:105])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Токенизация с помощью nltk</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(word_tokenize(text))\n",
    "# убрала пунктуацию и привела к нижнему регистру\n",
    "words = [w.lower() for w in word_tokenize(text) if w.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Парсинг с помощью pymorphy</h3>\n",
    "<p>Чтобы было удобнее потом сохранить результат в json, сразу перевожу данные в список из словарей</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "analist = []\n",
    "\n",
    "for word in words:\n",
    "    ana = morph.parse(word)\n",
    "    first = ana[0] # использую только 1-й вариант разбора\n",
    "\n",
    "    word = first.word # слово\n",
    "    tag = first.tag # грам. разбор\n",
    "    lemma = first.normal_form # лемма\n",
    "    prob = first.score # вероятность\n",
    "\n",
    "    ana_dict = {'text' : word, 'tag' : [tag], 'lemma' : lemma, 'prob' : prob}\n",
    "    analist.append(ana_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Чтобы можно было вытаскивать теги, не используя встроенные функции OpencorporaTag, добавила их отдельно в словарь, в список в значении \"tag\".</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'достоевский', 'tag': [OpencorporaTag('NOUN,anim,masc,Sgtm,Surn sing,nomn'), {'POS': 'NOUN', 'animacy': 'anim', 'case': 'nomn', 'gender': 'masc', 'number': 'sing'}], 'lemma': 'достоевский', 'prob': 1.0}, {'text': 'analysis', 'tag': [OpencorporaTag('LATN'), {}], 'lemma': 'analysis', 'prob': 1.0}, {'text': 'lex', 'tag': [OpencorporaTag('LATN'), {}], 'lemma': 'lex', 'prob': 1.0}, {'text': 'фёдор', 'tag': [OpencorporaTag('NOUN,anim,masc,Name sing,nomn'), {'POS': 'NOUN', 'animacy': 'anim', 'case': 'nomn', 'gender': 'masc', 'number': 'sing'}], 'lemma': 'фёдор', 'prob': 0.8}, {'text': 'gr', 'tag': [OpencorporaTag('LATN'), {}], 'lemma': 'gr', 'prob': 1.0}]\n"
     ]
    }
   ],
   "source": [
    "for parse in analist:\n",
    "    new_thing = {}\n",
    "    if parse['tag'][0].POS:\n",
    "        new_thing.update({'POS':parse['tag'][0].POS})\n",
    "    if parse['tag'][0].animacy:\n",
    "        new_thing.update({'animacy':parse['tag'][0].animacy})\n",
    "    if parse['tag'][0].aspect:\n",
    "        new_thing.update({'aspect':parse['tag'][0].aspect})\n",
    "    if parse['tag'][0].case:\n",
    "        new_thing.update({'case':parse['tag'][0].case})\n",
    "    if parse['tag'][0].gender:\n",
    "        new_thing.update({'gender':parse['tag'][0].gender})\n",
    "    if parse['tag'][0].involvement:\n",
    "        new_thing.update({'involvement':parse['tag'][0].involvement})\n",
    "    if parse['tag'][0].mood:\n",
    "        new_thing.update({'mood':parse['tag'][0].mood})\n",
    "    if parse['tag'][0].number:\n",
    "        new_thing.update({'number':parse['tag'][0].number})\n",
    "    if parse['tag'][0].person:\n",
    "        new_thing.update({'person':parse['tag'][0].person})\n",
    "    if parse['tag'][0].tense:\n",
    "        new_thing.update({'tense':parse['tag'][0].tense})\n",
    "    if parse['tag'][0].transitivity:\n",
    "        new_thing.update({'transitivity':parse['tag'][0].transitivity})\n",
    "    if parse['tag'][0].voice:\n",
    "        new_thing.update({'voice':parse['tag'][0].voice})\n",
    "    parse['tag'].append(new_thing)\n",
    "\n",
    "print(analist[100:105])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Пришлось оставить исходные значения \"tag\", так как не все грамматические значения относятся к какому-либо аттрибуту.</p>\n",
    "<p>Как результат, в значении \"tag\" список из двух елементов, 1-й - теги в формате OpencorporaTag, 2-й - словарь вида \"аттрибут : тег\".</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Сохранение в формате json</h3>\n",
    "<p>Формат json не признает OpencorporaTag, поэтому переделала тег в строку и убрала \"OpencorporaTag\", оставив только непосредственно теги</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('igrok_parse.json', 'w', encoding='utf-8') as f:\n",
    "\n",
    "    for parse in analist:\n",
    "        parse['tag'][0] = str(parse['tag'][0])\n",
    "        parse['tag'][0] = re.sub(r'OpencorporaTag()', '', parse['tag'][0])\n",
    "\n",
    "        json.dump(parse, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Проверка, что файл нормально записался, OpencorporaTag исчез.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'достоевский', 'tag': ['NOUN,anim,masc,Sgtm,Surn sing,nomn', {'POS': 'NOUN', 'animacy': 'anim', 'case': 'nomn', 'gender': 'masc', 'number': 'sing'}], 'lemma': 'достоевский', 'prob': 1.0}, {'text': 'analysis', 'tag': ['LATN', {}], 'lemma': 'analysis', 'prob': 1.0}, {'text': 'lex', 'tag': ['LATN', {}], 'lemma': 'lex', 'prob': 1.0}, {'text': 'фёдор', 'tag': ['NOUN,anim,masc,Name sing,nomn', {'POS': 'NOUN', 'animacy': 'anim', 'case': 'nomn', 'gender': 'masc', 'number': 'sing'}], 'lemma': 'фёдор', 'prob': 0.8}, {'text': 'gr', 'tag': ['LATN', {}], 'lemma': 'gr', 'prob': 1.0}]\n"
     ]
    }
   ],
   "source": [
    "ana_list = []\n",
    "with open('igrok_parse.json', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    lines = text.splitlines()\n",
    "\n",
    "for line in lines:\n",
    "    data = json.loads(line)\n",
    "    ana_list.append(data)\n",
    "\n",
    "print(ana_list[100:105])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Доля частей речи</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля NOUN 14.57%\n",
      "Доля PREP 2.22%\n",
      "Доля ADJF 2.59%\n",
      "Доля ADVB 1.56%\n",
      "Доля NPRO 2.29%\n",
      "Доля INFN 2.0%\n",
      "Доля GRND 0.88%\n",
      "Доля VERB 1.23%\n",
      "Доля NUMR 0.18%\n",
      "Доля CONJ 4.19%\n",
      "Доля PRCL 1.68%\n",
      "Доля INTJ 0.06%\n",
      "Доля PRED 0.13%\n",
      "Доля COMP 0.03%\n",
      "Доля ADJS 0.13%\n",
      "Доля PRTS 0.02%\n",
      "Доля PRTF 0.06%\n"
     ]
    }
   ],
   "source": [
    "word_count = len(analist) # количество слов в тексте\n",
    "poslist = [] # список частей речи\n",
    "\n",
    "# у некоторых слов отсутсвует часть речи (иноязычные слова, римские цифры)\n",
    "for parse in analist:\n",
    "    if 'POS' in parse['tag'][1].keys():\n",
    "        poslist.append(parse['tag'][1]['POS'])\n",
    "\n",
    "# считает количество вхождений\n",
    "posdict = Counter(poslist)\n",
    "# находит и выводит долю\n",
    "for pos, count in posdict.items():\n",
    "    print('Доля', pos, str(round(count/word_count * 100, 2))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Топ-20 глаголов и наречий</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-20 глаголов: ['быть', 'мочь', 'крупереть', 'знать', 'сказать', 'хотеть', 'начать', 'говорить', 'стать', 'думать', 'видеть', 'выйти', 'проиграть', 'заметить', 'дать', 'взять', 'понимать', 'любить', 'смотреть', 'отвечать']\n",
      "Топ-20 наречий: ['уж', 'тоже', 'уже', 'теперь', 'много', 'тут', 'вдруг', 'очень', 'почти', 'опять', 'здесь', 'потому', 'точно', 'тогда', 'действительно', 'тотчас', 'наконец', 'довольно', 'прямо', 'узко']\n"
     ]
    }
   ],
   "source": [
    "def top_pos(analist,tag):\n",
    "    lemmalist = [] # список лемм части речи\n",
    "    top_list = [] # список топ-20\n",
    "\n",
    "# находит лемму если в разборе соответсвующая часть речи   \n",
    "    for parse in analist:\n",
    "        if tag in parse['tag'][0]:\n",
    "            lemmalist.append(parse['lemma'])\n",
    "# cчитает вхождения\n",
    "    lemmadict = Counter(lemmalist)\n",
    "# находит 20 самых частотных\n",
    "    for key, value in lemmadict.most_common(20):\n",
    "        top_list.append(key)\n",
    "\n",
    "    return top_list\n",
    "    \n",
    "    \n",
    "print('Топ-20 глаголов:',top_pos(analist,'VERB'))\n",
    "print('Топ-20 наречий:',top_pos(analist,'ADVB'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>N-граммы</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(text, number):\n",
    "# делает список всех н-грамм\n",
    "    ngram_list = list(ngrams(text, number))\n",
    "# создает частотный словарь\n",
    "    ngrams_rank = Counter(ngram_list)\n",
    "# находит 25 самых частотных\n",
    "    top_20 = ngrams_rank.most_common(25)\n",
    "# убирает все лишние скобки\n",
    "    for ngram in top_20:\n",
    "        n_words = ' '.join(ngram[0])\n",
    "        frequency = ngram[1]\n",
    "        \n",
    "        print(n_words, frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Биграммы</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis lex 46207\n",
      "gr s 20511\n",
      "gr lex 19686\n",
      "ед text 12386\n",
      "gr text 12202\n",
      "ед ед 10183\n",
      "gr v 9281\n",
      "gr spro 8829\n",
      "пр ед 8770\n",
      "lex и 8356\n",
      "и gr 8356\n",
      "s пр 6139\n",
      "муж ед 5540\n",
      "вин ед 5277\n",
      "ед lex 4972\n",
      "ед сред 4649\n",
      "s муж 4623\n",
      "сред text 3890\n",
      "мн мн 3871\n",
      "ед изъяв 3835\n",
      "мн text 3642\n",
      "v несов 3598\n",
      "ед полн 3565\n",
      "s жен 3504\n",
      "lex я 3492\n"
     ]
    }
   ],
   "source": [
    "find_ngrams(words, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Триграммы</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lex и gr 8356\n",
      "ед ед ед 6291\n",
      "gr s пр 6139\n",
      "s пр ед 6138\n",
      "пр ед text 4900\n",
      "gr s муж 4623\n",
      "и gr lex 4178\n",
      "gr lex и 4178\n",
      "gr v несов 3598\n",
      "gr s жен 3504\n",
      "lex я gr 3492\n",
      "gr spro lex 3228\n",
      "мн мн мн 3120\n",
      "lex что gr 2913\n",
      "gr spro ед 2883\n",
      "spro ед сред 2661\n",
      "ед сред lex 2655\n",
      "gr spro text 2610\n",
      "пр ед lex 2581\n",
      "муж ед text 2549\n",
      "gr v ед 2526\n",
      "gr вин ед 2334\n",
      "s муж ед 2210\n",
      "ед изъяв муж 2179\n",
      "analysis lex я 2164\n"
     ]
    }
   ],
   "source": [
    "find_ngrams(words, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
